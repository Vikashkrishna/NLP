{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjy04zeT5bOb",
        "outputId": "9dba84a7-4ede-4a8e-f42f-15a501ab2b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "antonyms:The function antonyms(word) aims to retrieve antonyms for a given word by utilizing the WordNet lexical database. It first initializes an empty list anto to store the antonyms found. Then, it iterates over each synset of the input word in WordNet, exploring different senses or meanings associated with the word"
      ],
      "metadata": {
        "id": "5KS_I8HE9yCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "M9l_X2EK6FtL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def antonyms(word):\n",
        "  anto = []\n",
        "  for sys in wordnet.synsets(word):\n",
        "    for lem in sys.lemmas():\n",
        "      if lem.antonyms():\n",
        "        anto = lem.antonyms()\n",
        "  return anto\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9goHClMy6ON0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = input(\"Enter the word\")\n",
        "wordsys = antonyms(word)\n",
        "\n",
        "if(antonyms):\n",
        "  print(\"The antonym is \",wordsys)\n",
        "else:\n",
        "  print(\"The antonym for the word doesnot exist\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhFjyhR8osk",
        "outputId": "5f294beb-893f-42cb-ecd6-0c2e18ef4751"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the word\n",
            "The antonym is  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a program for stemming non-English words."
      ],
      "metadata": {
        "id": "MSbvi5NNM7Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Snowball Stemmer is an algorithm used for stemming words, which means reducing them to their root or base form."
      ],
      "metadata": {
        "id": "yyPdaaHMOaSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "xuFxVuPk8uqp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmer_english(words,language):\n",
        "  stemmer = SnowballStemmer(language)\n",
        "  stemm = stemmer.stem(words)\n",
        "  return stemm"
      ],
      "metadata": {
        "id": "jJBZE6358s2w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = input(\"Enter the language\")\n",
        "words = input(\"Enter the word\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruHt6d5PDyvL",
        "outputId": "7c6bcfd7-2457-4e58-d2c7-bc6e734ce3f6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the languagespanish\n",
            "Enter the wordPor favor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The stememed word is :\", stemmer_english(words,lang))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4e0_2GiE1eb",
        "outputId": "8c1f9493-bcb1-49ee-eeaf-62c3a4cf152c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The stememed word is : por favor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a program for lemmatizing words Using WordNet (Use all type of stemmers for the\n",
        "comparison)."
      ],
      "metadata": {
        "id": "kdLqws6UM4WS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WordNetLemmatizer determines the lemma, or base form, of a word based on its part of speech. The PorterStemmer and LancasterStemmer algorithms perform stemming by stripping affixes to obtain the root form, though LancasterStemmer tends to be more aggressive. The SnowballStemmer, an enhancement over Porter algorithm, offers improved accuracy and multilingual support, making it versatile for stemming tasks in NLP."
      ],
      "metadata": {
        "id": "JfiA2Ay5Oxn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n"
      ],
      "metadata": {
        "id": "BT8nI4RFFHQT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "kFOiZBCRGAKV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"played\", \"plays\", \"running\", \"ran\", \"playing\"]\n",
        "for word in words:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    porter_stem = porter_stemmer.stem(word)\n",
        "    lancaster_stem = lancaster_stemmer.stem(word)\n",
        "    snowball_stem = snowball_stemmer.stem(word)\n",
        "    print(f\"{word}\\t\\tlemmatised word:{lemmatized_word}\\t\\tporter stemming:{porter_stem}\\t\\tlancaster stemming:{lancaster_stem}\\t\\tSnowball stemming:{snowball_stem}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W73OqgPcGCws",
        "outputId": "9b000b03-d34d-4abc-f33c-fdee49622edf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\t\tlemmatised word:running\t\tporter stemming:run\t\tlancaster stemming:run\t\tSnowball stemming:run\n",
            "played\t\tlemmatised word:played\t\tporter stemming:play\t\tlancaster stemming:play\t\tSnowball stemming:play\n",
            "plays\t\tlemmatised word:play\t\tporter stemming:play\t\tlancaster stemming:play\t\tSnowball stemming:play\n",
            "running\t\tlemmatised word:running\t\tporter stemming:run\t\tlancaster stemming:run\t\tSnowball stemming:run\n",
            "ran\t\tlemmatised word:ran\t\tporter stemming:ran\t\tlancaster stemming:ran\t\tSnowball stemming:ran\n",
            "playing\t\tlemmatised word:playing\t\tporter stemming:play\t\tlancaster stemming:play\t\tSnowball stemming:play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a program to differentiate stemming and lemmatizing words."
      ],
      "metadata": {
        "id": "aIC9N48IMpF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"The quick brown foxes are running in the fields.\",\n",
        "    \"He played football yesterday and he is playing again today.\"\n",
        "]\n",
        "\n",
        "print(\"Sentence\\t\\tLemmatized\\t\\tStemmed\")\n",
        "\n",
        "\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "\n",
        "    print(f\"{sentence}\\n\")\n",
        "    for i in range(len(words)):\n",
        "        print(f\"{words[i]:15} Lemmatised word:{lemmatized_words[i]:15} Stemmed word:{stemmed_words[i]:15}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3d-RLJsGU5Y",
        "outputId": "aed2e231-82eb-4b40-a79b-61ab587529c6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence\t\tLemmatized\t\tStemmed\n",
            "The quick brown foxes are running in the fields.\n",
            "\n",
            "The             Lemmatised word:The             Stemmed word:the            \n",
            "quick           Lemmatised word:quick           Stemmed word:quick          \n",
            "brown           Lemmatised word:brown           Stemmed word:brown          \n",
            "foxes           Lemmatised word:fox             Stemmed word:fox            \n",
            "are             Lemmatised word:are             Stemmed word:are            \n",
            "running         Lemmatised word:running         Stemmed word:run            \n",
            "in              Lemmatised word:in              Stemmed word:in             \n",
            "the             Lemmatised word:the             Stemmed word:the            \n",
            "fields          Lemmatised word:field           Stemmed word:field          \n",
            ".               Lemmatised word:.               Stemmed word:.              \n",
            "He played football yesterday and he is playing again today.\n",
            "\n",
            "He              Lemmatised word:He              Stemmed word:he             \n",
            "played          Lemmatised word:played          Stemmed word:play           \n",
            "football        Lemmatised word:football        Stemmed word:footbal        \n",
            "yesterday       Lemmatised word:yesterday       Stemmed word:yesterday      \n",
            "and             Lemmatised word:and             Stemmed word:and            \n",
            "he              Lemmatised word:he              Stemmed word:he             \n",
            "is              Lemmatised word:is              Stemmed word:is             \n",
            "playing         Lemmatised word:playing         Stemmed word:play           \n",
            "again           Lemmatised word:again           Stemmed word:again          \n",
            "today           Lemmatised word:today           Stemmed word:today          \n",
            ".               Lemmatised word:.               Stemmed word:.              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.It uses the pos_tag function from NLTK to perform POS tagging on the list of words. The Averaged Perceptron tagger model is used to assign a tag to each word. The result is stored in the tagged_words variable."
      ],
      "metadata": {
        "id": "-zXmdwr5J22q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech (POS) tagging is the process of labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc."
      ],
      "metadata": {
        "id": "1uzUuBv9O4qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Vikash is an outstanding student in the field of artificial intelligence and machine learning. With his dedication and passion, he explores the depths of AI and ML, unraveling complex algorithms and techniques. His innovative mindset and analytical skills set him apart, driving his towards groundbreaking discoveries in the realm of technology. As an AI and ML enthusiast, vikash continues to inspire his peers with his relentless pursuit of knowledge and his commitment to pushing the boundaries of what is possible in the ever-evolving world of AI and ML.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "print(tagged_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eDl2fG1JdsQ",
        "outputId": "c9c77d1f-5e8d-4e7d-a049-e0595f24e62f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Vikash', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('outstanding', 'JJ'), ('student', 'NN'), ('in', 'IN'), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.'), ('With', 'IN'), ('his', 'PRP$'), ('dedication', 'NN'), ('and', 'CC'), ('passion', 'NN'), (',', ','), ('he', 'PRP'), ('explores', 'VBZ'), ('the', 'DT'), ('depths', 'NNS'), ('of', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), (',', ','), ('unraveling', 'VBG'), ('complex', 'JJ'), ('algorithms', 'NN'), ('and', 'CC'), ('techniques', 'NNS'), ('.', '.'), ('His', 'PRP$'), ('innovative', 'JJ'), ('mindset', 'NN'), ('and', 'CC'), ('analytical', 'JJ'), ('skills', 'NNS'), ('set', 'VBD'), ('him', 'PRP'), ('apart', 'RB'), (',', ','), ('driving', 'VBG'), ('his', 'PRP$'), ('towards', 'NNS'), ('groundbreaking', 'VBG'), ('discoveries', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('realm', 'NN'), ('of', 'IN'), ('technology', 'NN'), ('.', '.'), ('As', 'IN'), ('an', 'DT'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), ('enthusiast', 'NN'), (',', ','), ('vikash', 'NN'), ('continues', 'VBZ'), ('to', 'TO'), ('inspire', 'VB'), ('his', 'PRP$'), ('peers', 'NNS'), ('with', 'IN'), ('his', 'PRP$'), ('relentless', 'NN'), ('pursuit', 'NN'), ('of', 'IN'), ('knowledge', 'NN'), ('and', 'CC'), ('his', 'PRP$'), ('commitment', 'NN'), ('to', 'TO'), ('pushing', 'VBG'), ('the', 'DT'), ('boundaries', 'NNS'), ('of', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('possible', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('ever-evolving', 'JJ'), ('world', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.The sentence is tokenized and the words are POS tagged and the NER(Named Entity Recognition) is performed using ne_chunk function."
      ],
      "metadata": {
        "id": "LCTRvBqPKpTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER) is a natural language processing task that involves identifying and classifying named entities within text into predefined categories such as names of persons, organizations, locations, dates, and more"
      ],
      "metadata": {
        "id": "0bCnxcN0O-3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "words = word_tokenize(sentence)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "named_entities = ne_chunk(tagged_words)\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zglwKpapKqTT",
        "outputId": "39d83d0f-b63d-4bff-b462-5e248a126e47"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Vikash/NNP)\n",
            "  is/VBZ\n",
            "  an/DT\n",
            "  outstanding/JJ\n",
            "  student/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  artificial/JJ\n",
            "  intelligence/NN\n",
            "  and/CC\n",
            "  machine/NN\n",
            "  learning/NN\n",
            "  ./.\n",
            "  With/IN\n",
            "  his/PRP$\n",
            "  dedication/NN\n",
            "  and/CC\n",
            "  passion/NN\n",
            "  ,/,\n",
            "  he/PRP\n",
            "  explores/VBZ\n",
            "  the/DT\n",
            "  depths/NNS\n",
            "  of/IN\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  and/CC\n",
            "  (ORGANIZATION ML/NNP)\n",
            "  ,/,\n",
            "  unraveling/VBG\n",
            "  complex/JJ\n",
            "  algorithms/NN\n",
            "  and/CC\n",
            "  techniques/NNS\n",
            "  ./.\n",
            "  His/PRP$\n",
            "  innovative/JJ\n",
            "  mindset/NN\n",
            "  and/CC\n",
            "  analytical/JJ\n",
            "  skills/NNS\n",
            "  set/VBD\n",
            "  him/PRP\n",
            "  apart/RB\n",
            "  ,/,\n",
            "  driving/VBG\n",
            "  his/PRP$\n",
            "  towards/NNS\n",
            "  groundbreaking/VBG\n",
            "  discoveries/NNS\n",
            "  in/IN\n",
            "  the/DT\n",
            "  realm/NN\n",
            "  of/IN\n",
            "  technology/NN\n",
            "  ./.\n",
            "  As/IN\n",
            "  an/DT\n",
            "  AI/NNP\n",
            "  and/CC\n",
            "  (ORGANIZATION ML/NNP)\n",
            "  enthusiast/NN\n",
            "  ,/,\n",
            "  vikash/NN\n",
            "  continues/VBZ\n",
            "  to/TO\n",
            "  inspire/VB\n",
            "  his/PRP$\n",
            "  peers/NNS\n",
            "  with/IN\n",
            "  his/PRP$\n",
            "  relentless/NN\n",
            "  pursuit/NN\n",
            "  of/IN\n",
            "  knowledge/NN\n",
            "  and/CC\n",
            "  his/PRP$\n",
            "  commitment/NN\n",
            "  to/TO\n",
            "  pushing/VBG\n",
            "  the/DT\n",
            "  boundaries/NNS\n",
            "  of/IN\n",
            "  what/WP\n",
            "  is/VBZ\n",
            "  possible/JJ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  ever-evolving/JJ\n",
            "  world/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  and/CC\n",
            "  (ORGANIZATION ML/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing involves analyzing the syntactic structure of a sentence by identifying the relationships between words.Constituency parsing involves analyzing the syntactic structure of a sentence by breaking it down into smaller, nested phrases or constituents"
      ],
      "metadata": {
        "id": "iiAfRbtKPNUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "sentences = [\"My name is vikash krishna.I am studying MSAIML\"]\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    doc = nlp(sentence)\n",
        "    print(f\"Depdendency Parse for Sentence {i+1}:\")\n",
        "    displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "WGv0y7eqK0Bb",
        "outputId": "9ee16725-3ba7-454a-9192-0113595adf43"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depdendency Parse for Sentence 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"011badc65d014d70a53a1ce9c541a2a3-0\" class=\"displacy\" width=\"860\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">My</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">name</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">vikash</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">krishna.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">am</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">studying</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">MSAIML</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,47.0 135.0,47.0 135.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-1\" stroke-width=\"2px\" d=\"M160,92.0 C160,47.0 225.0,47.0 225.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,94.0 L152,82.0 168,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-2\" stroke-width=\"2px\" d=\"M340,92.0 C340,47.0 405.0,47.0 405.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,94.0 L332,82.0 348,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-3\" stroke-width=\"2px\" d=\"M250,92.0 C250,2.0 410.0,2.0 410.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M410.0,94.0 L418.0,82.0 402.0,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-4\" stroke-width=\"2px\" d=\"M520,92.0 C520,2.0 680.0,2.0 680.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520,94.0 L512,82.0 528,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-5\" stroke-width=\"2px\" d=\"M610,92.0 C610,47.0 675.0,47.0 675.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M610,94.0 L602,82.0 618,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-011badc65d014d70a53a1ce9c541a2a3-0-6\" stroke-width=\"2px\" d=\"M700,92.0 C700,47.0 765.0,47.0 765.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-011badc65d014d70a53a1ce9c541a2a3-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M765.0,94.0 L773.0,82.0 757.0,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tree import Tree\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "for sent in doc.sents:\n",
        "    tree = to_nltk_tree(sent.root)\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MCXpitZKnqy",
        "outputId": "2f34cd84-e571-4225-b19b-b94a68f512ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     is         \n",
            "  ___|______     \n",
            " |  name krishna\n",
            " |   |      |    \n",
            " .   My   vikash\n",
            "\n",
            "    studying       \n",
            "  _____|_______     \n",
            " I     am    MSAIML\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzrv92_F-2sW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}